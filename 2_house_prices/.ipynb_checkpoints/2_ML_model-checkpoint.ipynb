{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adb506d-5318-4d4f-b055-08cd89618f51",
   "metadata": {},
   "source": [
    "# House Price Prediction\n",
    "\n",
    "The goal of this project is to predict the sales price of residential homes in Ames, Iowa, USA based on various of attributes. It is a supervised regression problem.\n",
    "\n",
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3be7ec1-1ba8-475e-a0e6-4bfd8d0f7bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV, LassoCV, RidgeCV, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df9715-96fb-4a55-95b7-99e5be5604c2",
   "metadata": {},
   "source": [
    "# 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01708925-8eac-418a-9232-773c83091304",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_cleaned_2.csv\", header=0)\n",
    "test = pd.read_csv(\"data/test_cleaned_2.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797aa12-2834-4740-ab96-f22c8393bc26",
   "metadata": {},
   "source": [
    "# 6. Choose an Evaluation Metrics\n",
    "For regression problems, common metrics include:\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- R-squared\n",
    "\n",
    "For classification problems, common metrics include:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score\n",
    "- AUC\n",
    "\n",
    "The confusion matric and ROC Curve can also bring useful insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f814a0f9-d5b8-445a-8b99-80d99f19ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return root_mean_squared_error(y, y_pred)\n",
    "\n",
    "def cv_rmse(model, X, y):\n",
    "    return np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50089933-f10f-4fb7-8283-7e50d2a519d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning model\n",
    "\n",
    "X_train = train.drop(columns=['SalePrice', 'Id'])\n",
    "X_test = train.drop(columns=['Id'])\n",
    "y_train = train['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a741ef0-de39-4092-9f07-67cb983fd46c",
   "metadata": {},
   "source": [
    "# 7. Select Algorithms\n",
    "- Start with simple regression algorithms like Linear Regression and gradually explore more complex models like Random Forest, Gradient Boosting, or XGBoost.\n",
    "- Consider simple ensemble methods, such as simple average, weighted average, or voting ensembles, to combine multiple models for potentially better results.\n",
    "- The model chosen depends on the data. A more complex model does not always constitute a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee226cd-66c6-45e8-a647-49f1ba3c3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710501bb-3240-482b-af1a-f506095320d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    Lasso(),\n",
    "    ElasticNet(),\n",
    "    SGDRegressor(),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    SVR(),\n",
    "    KNeighborsRegressor()\n",
    "]\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'SGD Regressor': SGDRegressor(max_iter=1000, tol=1e-3), # Often needs more iterations\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42), # Set seed for reproducibility\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'K-Neighbors': KNeighborsRegressor()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e577940a-6ce6-4b18-b000-5bf1a7758b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n",
    "lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n",
    "elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \n",
    "sdrg = SGDRegressor()\n",
    "dtr = DecisionTreeRegressor()\n",
    "rfr = RandomForestRegressor()\n",
    "knr = KNeighborsRegressor()\n",
    "svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=3000, \n",
    "    learning_rate=0.05, \n",
    "    max_depth=4, \n",
    "    max_features='sqrt', \n",
    "    min_samples_leaf=15, \n",
    "    min_samples_split=10, \n",
    "    loss='huber', \n",
    "    random_state=42)\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                         num_leaves=4,\n",
    "                         learning_rate=0.01,\n",
    "                         n_estimators=5000,\n",
    "                         max_bin=200,\n",
    "                         bagging_fraction=0.75,\n",
    "                         bagging_freq=5,\n",
    "                         bagging_seed=7,\n",
    "                         feature_fraction=0.2,\n",
    "                         feature_fraction_seed=7,\n",
    "                         verbose=-1)\n",
    "xgboost = XGBRegressor(learning_rate=0.01,\n",
    "                       n_estimators=3460,\n",
    "                       max_depth=3, \n",
    "                       min_child_weight=0,\n",
    "                       gamma=0, \n",
    "                       subsample=0.7,\n",
    "                       colsample_bytree=0.7,\n",
    "                       objective='reg:linear', \n",
    "                       nthread=-1,\n",
    "                       scale_pos_weight=1, \n",
    "                       seed=27,\n",
    "                       reg_alpha=0.00006)\n",
    "stack_gen = StackingCVRegressor(\n",
    "    regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n",
    "    meta_regressor=xgboost,\n",
    "    use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebe25d-1537-4a4d-9b0b-f5bf8633fdc7",
   "metadata": {},
   "source": [
    "# 8. Model Validation\n",
    "- Split the data into training and validation sets. A common split is 70-30 or 80-20 for training and validation, respectively. This method is computationally less intensive and often used for initial model exploration or when dealing with very large datasets.\n",
    "- K-Fold Cross Validation. This method provides a more reliable evaluation, especially with smaller datasets.\n",
    "- Model validation is important to assess the model's generalization performance (i.e. assess how well the model performs on unseen data). This helps prevent overfitting and gives you a more reliable estimate of your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "610821b4-e015-442a-8a31-fd09981ddd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in X_train: 13421\n",
      "Inf values in X_train: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"NaN values in X_train: {np.isnan(X_test).sum().sum()}\")\n",
    "print(f\"Inf values in X_train: {np.isinf(X_test).sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a97081d-816b-4744-acdb-dbefa6b53b69",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 2191, in fit\n    return super().fit(X, y, sample_weight=sample_weight, **params)\n           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n    estimator._validate_params()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n    validate_parameter_constraints(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        self._parameter_constraints,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.get_params(deep=False),\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        caller_name=self.__class__.__name__,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\n    ...<2 lines>...\n    )\nsklearn.utils._param_validation.InvalidParameterError: The 'max_iter' parameter of LassoCV must be an int in the range [1, inf). Got 10000000.0 instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m score = \u001b[43mcv_rmse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlasso\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLASSO: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(score.mean(), score.std()), datetime.now(), )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcv_rmse\u001b[39m\u001b[34m(model, X, y)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcv_rmse\u001b[39m(model, X, y):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(-\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mneg_mean_squared_error\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkfolds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:677\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    675\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:419\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    399\u001b[39m results = parallel(\n\u001b[32m    400\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    401\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    417\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/pipeline.py\", line 663, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py\", line 2191, in fit\n    return super().fit(X, y, sample_weight=sample_weight, **params)\n           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n    estimator._validate_params()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n    validate_parameter_constraints(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        self._parameter_constraints,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        self.get_params(deep=False),\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n        caller_name=self.__class__.__name__,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/home/pierre/bin/miniconda3/envs/py-data/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\n    ...<2 lines>...\n    )\nsklearn.utils._param_validation.InvalidParameterError: The 'max_iter' parameter of LassoCV must be an int in the range [1, inf). Got 10000000.0 instead.\n"
     ]
    }
   ],
   "source": [
    "score = cv_rmse(lasso, X_train, y_train)\n",
    "print(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31cb76d-9e73-4896-8d98-52ef8210bb7e",
   "metadata": {},
   "source": [
    "## 8.1. Hyperparameter Tuning\n",
    "- Tune the hyperparameters of your chosen algorithms on the validation dataset using techniques like grid search or random search to find the best combination.\n",
    "- Optuna is an efficient and effective way to search for optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69922622-d19e-4d06-8c01-2dc3b3d10c52",
   "metadata": {},
   "source": [
    "## 8.2. Regularization\n",
    "- Implement regularization techniques like L1 (Lasso) or L2 (Ridge) regularization to prevent overfitting.\n",
    "- Many ML algorithms include regularization parameters, including L1 and L2, sometimes called reg_alpha or reg_lambda. Read up on your chosen algorithms regularization parameters and tune them accordingly on your validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7e197-0897-4ffb-8429-c481462e7f48",
   "metadata": {},
   "source": [
    "# 9. Train the final model\n",
    "\n",
    "- Fit the best model using the optimal hyperparameters found on the whole training set (including the validation set)\n",
    "- Model persistence : save the model weights for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e01c0-a54f-4d27-b78b-fc5a6f7f7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(model, X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Train a classification model and assessing performance\n",
    "    model: eg. model = LogisticRegression()\n",
    "    X_train: train dataframe without the target column\n",
    "    X_test: test dataframe    \n",
    "    y_train: target column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use model class name as model name\n",
    "    model_name = model.__class__.__name__\n",
    "    print(f\"Training: {model_name}\")    \n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Perform cross-validation with 5 folds\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    cv_mean = np.mean(scores)\n",
    "    cv_std = np.std(scores)\n",
    "    \n",
    "    # Predictions on train set\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    # Predict on test set \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    precision = metrics.precision_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
    "    recall = metrics.recall_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
    "    f1 = metrics.f1_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results_dict = {\n",
    "        'Model_Name': model_name,\n",
    "        'Train_Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1,\n",
    "        'CV_Mean': cv_mean,\n",
    "        'CV_Std': cv_std\n",
    "    }\n",
    "    \n",
    "    return y_pred_test, y_pred_train, results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687ca94-d9c8-43eb-a567-5c526a3d9556",
   "metadata": {},
   "source": [
    "# 10. Generate predictions on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3cf95-4f6d-44d9-b714-c7d79bd53ba9",
   "metadata": {},
   "source": [
    "# 11. Save Model weights (model persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e02ea-ea18-44de-b3a3-52eefad28575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
