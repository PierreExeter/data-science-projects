{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adb506d-5318-4d4f-b055-08cd89618f51",
   "metadata": {},
   "source": [
    "# Project Name\n",
    "\n",
    "[Project Description]\n",
    "\n",
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be7ec1-1ba8-475e-a0e6-4bfd8d0f7bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48df9715-96fb-4a55-95b7-99e5be5604c2",
   "metadata": {},
   "source": [
    "# 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01708925-8eac-418a-9232-773c83091304",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058b668-bab1-4a6c-97c1-aa109bc33ba0",
   "metadata": {},
   "source": [
    "# 2. Understand the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19e4cc-3417-490e-bf00-c0935b46fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_df.head(5))\n",
    "display(test_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501a7fa-fbde-4752-9360-7abd9841c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_df.info())\n",
    "display(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece6a3d-e20e-47c1-985a-5c2d9c8e9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2833e-a7da-41e1-8363-13f7b7ef8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6767075-df92-46f7-942e-936e9ea65378",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "\n",
    "## 3.1. Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b665d-76ee-48a7-945e-2f10a176418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a22dc2-2b47-4d78-acc1-a63ad56e4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5cab2b-b8d9-4694-b419-52a8fcd1139f",
   "metadata": {},
   "source": [
    "## 3.2. Check for missing data\n",
    "Let's check for 0, blank, NaN or None values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b16b2f-009c-4576-8185-59a2356698e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    (train_df == 0).sum().rename('zeros'),\n",
    "    (train_df == '').sum().rename('blanks'), \n",
    "    train_df.isna().sum().rename('nan'),\n",
    "    (train_df == None).sum().rename('none')\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a705054-b616-413a-af00-8215b6e9bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    (test_df == 0).sum().rename('zeros'),\n",
    "    (test_df == '').sum().rename('blanks'), \n",
    "    test_df.isna().sum().rename('nan'),\n",
    "    (test_df == None).sum().rename('none')\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b4295-f8da-49e0-b43a-0aaae3c9290d",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "## 4.1. Univariate analysis\n",
    "\n",
    "For each categorical variable, display the bar plot.\n",
    "\n",
    "For each numerical variable, show histograms, measures of central tendency (mean, median, mode), and measures of dispersion (range, standard deviation, skewness, kurtosis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7cd591-829d-43c7-81a7-66f9792c2c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18c53ea1-9cb3-4e6a-923d-26aa7a658b59",
   "metadata": {},
   "source": [
    "## 4.2. Bivariate analysis\n",
    "\n",
    "Understand the relationships between features and the target variable using scatterplots, correlation coefficients / matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5f085-c07b-4629-88a6-63c70f9dd7ad",
   "metadata": {},
   "source": [
    "## 4.3. Handle Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18df6af-165c-4a71-ac9a-399b343c4f90",
   "metadata": {},
   "source": [
    "## 4.4. Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84204f8b-d89d-4654-939b-43b9b4a03981",
   "metadata": {},
   "source": [
    "# 5. Feature engineering\n",
    "\n",
    "## 5.1. Create new features\n",
    "\n",
    "Let's engineer relevant features that might improve predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3c253-b71c-4b4a-b6ff-82f965e6e38a",
   "metadata": {},
   "source": [
    "## 5.2 Remove irrelevant features\n",
    "Let's eliminate features that do not contribute much to the prediction. Many ML algorithms, such as Random Forest, provide a feature importance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aca1d0-0406-4d61-a362-4e7918e1243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df5179-a672-4d98-984f-5d2439df48e5",
   "metadata": {},
   "source": [
    "## 5.3. Feature Normalization\n",
    "\n",
    "Many models assume normally distributed data. Let's fix skewed features by applying log transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a409d-ebdc-4cf7-96ec-ac5e18878f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in quantitative:\n",
    "    train[col] = np.log1p(train[col])\n",
    "    test[col] = np.log1p(test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a066e2-26b5-4a43-8ccd-1d97943b6a43",
   "metadata": {},
   "source": [
    "## 5.4. Feature Scaling\n",
    "\n",
    "Scaling numerical features improves distance-based calculations (for KNN, SVM classifiers) and prevents feature dominance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea6dae-e95a-4673-846f-076e4687ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df):\n",
    "    \"\"\" scale numerical features \"\"\"\n",
    "\n",
    "    mew = df[quantitative].mean(axis=0)\n",
    "    std = df[quantitative].std(axis=0)\n",
    "    df[quantitative] = (df[quantitative] - mew) / std\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e5f07-9fb2-47a5-a74d-0d69973c981f",
   "metadata": {},
   "source": [
    "## 5.5. Remove colinear features\n",
    "\n",
    "This improves the model's stability and interpretability, and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90aa67c-4ec5-4c2f-86db-375aac3e173d",
   "metadata": {},
   "source": [
    "## 5.6. Encode categorical features\n",
    "\n",
    "Let's convert categorical features into numerical form using techniques like one-hot encoding or label encoding.\n",
    "\n",
    "Note : using drop_first=True creates 1 column instead of 2 for 2 categories (dummy variable trapping). This avoids multicollinearity between columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8f617-cd84-4c9b-ae50-e2b7656f26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df):\n",
    "    \"\"\" \n",
    "    Perform one hot encoding on features Sex, Pclass, Deck, Embarked and Title. \n",
    "    Concatenate to the main dataframe.\n",
    "    \"\"\"\n",
    "    df_sex = pd.get_dummies(df['Sex'], prefix='sex', drop_first=True, dtype=int)\n",
    "    df_Pclass = pd.get_dummies(df['Pclass'], prefix='class', drop_first=True, dtype=int)\n",
    "    df_Embarked = pd.get_dummies(df['Embarked'], prefix='Embarked', drop_first=True, dtype=int)\n",
    "    df_Deck = pd.get_dummies(df['Deck'], prefix='Deck', drop_first=False, dtype=int)\n",
    "    df_Title = pd.get_dummies(df['Title'], prefix='Title', drop_first=False, dtype=int)\n",
    "\n",
    "    df = pd.concat([df, df_sex, df_Pclass, df_Embarked, df_Deck, df_Title], axis=1)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797aa12-2834-4740-ab96-f22c8393bc26",
   "metadata": {},
   "source": [
    "# 6. Choose an Evaluation Metrics\n",
    "For regression problems, common metrics include:\n",
    "- Mean Absolute Error (MAE)\n",
    "- Mean Squared Error (MSE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- R-squared\n",
    "\n",
    "For classification problems, common metrics include:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score\n",
    "- AUC\n",
    "\n",
    "The confusion matric and ROC Curve can also bring useful insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a741ef0-de39-4092-9f07-67cb983fd46c",
   "metadata": {},
   "source": [
    "# 7. Select Algorithms\n",
    "- Start with simple regression algorithms like Linear Regression and gradually explore more complex models like Random Forest, Gradient Boosting, or XGBoost.\n",
    "- Consider simple ensemble methods, such as simple average, weighted average, or voting ensembles, to combine multiple models for potentially better results.\n",
    "- The model chosen depends on the data. A more complex model does not always constitute a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ebe25d-1537-4a4d-9b0b-f5bf8633fdc7",
   "metadata": {},
   "source": [
    "# 8. Model Validation\n",
    "- Split the data into training and validation sets. A common split is 70-30 or 80-20 for training and validation, respectively. This method is computationally less intensive and often used for initial model exploration or when dealing with very large datasets.\n",
    "- K-Fold Cross Validation. This method provides a more reliable evaluation, especially with smaller datasets.\n",
    "- Model validation is important to assess the model's generalization performance (i.e. assess how well the model performs on unseen data). This helps prevent overfitting and gives you a more reliable estimate of your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31cb76d-9e73-4896-8d98-52ef8210bb7e",
   "metadata": {},
   "source": [
    "# 9. Hyperparameter Tuning\n",
    "- Tune the hyperparameters of your chosen algorithms on the validation dataset using techniques like grid search or random search to find the best combination.\n",
    "- Optuna is an efficient and effective way to search for optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69922622-d19e-4d06-8c01-2dc3b3d10c52",
   "metadata": {},
   "source": [
    "# 10. Regularization\n",
    "- Implement regularization techniques like L1 (Lasso) or L2 (Ridge) regularization to prevent overfitting.\n",
    "- Many ML algorithms include regularization parameters, including L1 and L2, sometimes called reg_alpha or reg_lambda. Read up on your chosen algorithms regularization parameters and tune them accordingly on your validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7e197-0897-4ffb-8429-c481462e7f48",
   "metadata": {},
   "source": [
    "# 11. Train the final model\n",
    "\n",
    "- Fit the best model using the optimal hyperparameters found on the whole training set (including the validation set)\n",
    "- Model persistence : save the model weights for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e01c0-a54f-4d27-b78b-fc5a6f7f7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(model, X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Train a classification model and assessing performance\n",
    "    model: eg. model = LogisticRegression()\n",
    "    X_train: train dataframe without the target column\n",
    "    X_test: test dataframe    \n",
    "    y_train: target column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use model class name as model name\n",
    "    model_name = model.__class__.__name__\n",
    "    print(f\"Training: {model_name}\")    \n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Perform cross-validation with 5 folds\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    cv_mean = np.mean(scores)\n",
    "    cv_std = np.std(scores)\n",
    "    \n",
    "    # Predictions on train set\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    # Predict on test set \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = metrics.accuracy_score(y_train, y_pred_train)\n",
    "    precision = metrics.precision_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
    "    recall = metrics.recall_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
    "    f1 = metrics.f1_score(y_train, y_pred_train, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results_dict = {\n",
    "        'Model_Name': model_name,\n",
    "        'Train_Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1_Score': f1,\n",
    "        'CV_Mean': cv_mean,\n",
    "        'CV_Std': cv_std\n",
    "    }\n",
    "    \n",
    "    return y_pred_test, y_pred_train, results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687ca94-d9c8-43eb-a567-5c526a3d9556",
   "metadata": {},
   "source": [
    "# 12. Generate predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06b60c-9e4d-4f05-86ea-7aed27d5b197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e02ea-ea18-44de-b3a3-52eefad28575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
